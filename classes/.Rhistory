ggpubr,
ggplot2,
stringr)
install.packages("rstanarm")
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)
ggplot(data = NULL, aes(x = my_simulated_data)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(title = "Histogram of Simulated Data",
x = "Value",
y = "Frequency")
ggplot(data.frame(x = data), aes(x)) +
geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black") +
geom_density(alpha = .2, fill = "#FF6666") +
labs(title = "Histogram of Simulated Data", x = "Value", y = "Density")
ggplot(data.frame(x = my_simulated_data), aes(x)) +
geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black") +
geom_density(alpha = .2, fill = "#FF6666") +
labs(title = "Histogram of Simulated Data", x = "Value", y = "Density")
ggplot(data = NULL, aes(x = my_simulated_data)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(title = "Histogram of Simulated Data",
x = "Value",
y = "Frequency")
mean(my_simulated_data)
sd(my_simulated_data)
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
error <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept+slope*x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = 3.9)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
# 6) Now, plot to check it out
plot(df)
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y ~ x, data=df)
coef(M2)
abline(coef(M2))
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none")
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y ~ x, data=df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")
# Display the plot
p
# Now, let's plot
p <- ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")  # Ensure the correct y aesthetic is used
# Display the plot
p
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
error <- 3.9
nd_random_error <- rnorm(20, mean = 0, sd = error)
nd_random_error <- rnorm(20, mean = 0, sd = error)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y ~ x, data=df)
coef(M2)
abline(coef(M2))
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y, y_witherror = y_witherror)
# 6) Now, plot to check it out
plot(df)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
# 6) Now, plot to check it out
plot(df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")
# 6) Now, plot to check it out
plot(df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")
# 6) Now, plot to check it out
plot(df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", color = "black")                    #### --> HERE 2*
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE 2*
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none")
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE *
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
print(M12a) # We see 3.9 under sigma indeed
M2 <- stan_glm(y_witherror ~ x, data=df)
print(M2)
coef(M2)
abline(coef(M2))
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/paulasamide/Desktop/AU/semester 2/methods/resources/classes")
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
install.packages("rstanarm")
library(rstanarm)
# Load the rest
library(pacman)
pacman::p_load(tidyverse,
ggpubr,
ggplot2,
stringr) # this time I'm just giving you the code
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)
# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")
text(hibbs$growth, hibbs$vote, labels = hibbs$year, pos = 3, cex = 0.8)
# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)
# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way
# Display the fitted model
print(M1)
# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote))+
geom_point() +  # Add points
labs(
x = "Average recent growth in personal income",
y = "Incumbent party's vote share",
title = "Relationship between Income Growth and Vote Share",
subtitle = "Data from Hibbs Dataset"
)+
theme_minimal() +  # Use a minimal theme
theme(
plot.title = element_text(hjust = 0.5),  # Center the title
plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
) +
geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line
set.seed(1998) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed
# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!
# your code here
my_simulated_data <- rnorm(20, mean = 10, sd = 5)
ggplot(data = NULL, aes(x = my_simulated_data)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(title = "Histogram of Simulated Data",
x = "Value",
y = "Frequency")
mean(my_simulated_data)
sd(my_simulated_data)
install.packages("rstanarm")
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE *
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
print(M12a) # We see 3.9 under sigma indeed
M2 <- stan_glm(y_witherror ~ x, data=df)
print(M2)
coef(M2)
abline(coef(M2))
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
# ggplot(data, aes(x = x)) +
geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
labs(
x = 'Average recent growth in personal income (%)',
y = "Incumbent party's vote share",
title = 'Hypothetical data and fitted line',
subtitle = 'Fitted line: y = 30 + 10x'
) +
theme_minimal() +
theme(legend.position = "none") +
geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE *
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
# ggplot(data, aes(x = x)) +
#  geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
#  geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
#  labs(
#    x = 'Average recent growth in personal income (%)',
#    y = "Incumbent party's vote share",
#    title = 'Hypothetical data and fitted line',
#    subtitle = 'Fitted line: y = 30 + 10x'
#  ) +
#  theme_minimal() +
#  theme(legend.position = "none") +
#    geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE *
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
print(M12a) # We see 3.9 under sigma indeed
M2 <- stan_glm(y_witherror ~ x, data=df)
print(M2)
coef(M2)
abline(coef(M2))
M3 <- stan_glm(y ~ x, data=df)
print(M3)
# Fit a model for fun to see
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
print(M12a) # We see 3.9 under sigma indeed
M13a <- stan_glm(y_line ~ x, data = data, refresh = 0)
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/paulasamide/Desktop/AU/semester 2/methods/resources/classes")
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/paulasamide/Desktop/AU/semester 2/methods/resources/classes")
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
library(rstanarm)
# Load the rest
library(pacman)
pacman::p_load(tidyverse,
ggpubr,
ggplot2,
stringr) # this time I'm just giving you the code
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)
# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")
text(hibbs$growth, hibbs$vote, labels = hibbs$year, pos = 3, cex = 0.8)
# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)
# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way
# Display the fitted model
print(M1)
# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote))+
geom_point() +  # Add points
labs(
x = "Average recent growth in personal income",
y = "Incumbent party's vote share",
title = "Relationship between Income Growth and Vote Share",
subtitle = "Data from Hibbs Dataset"
)+
theme_minimal() +  # Use a minimal theme
theme(
plot.title = element_text(hjust = 0.5),  # Center the title
plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
) +
geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line
set.seed(1998) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed
# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!
# your code here
my_simulated_data <- rnorm(20, mean = 10, sd = 5)
ggplot(data = NULL, aes(x = my_simulated_data)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(title = "Histogram of Simulated Data",
x = "Value",
y = "Frequency")
mean(my_simulated_data)
sd(my_simulated_data)
#### 1) define the regression model parameters (intercept, slope, error)      ----- **WHAT IS THE ERROR HERE FOR?**
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(from = 0, to = 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
data <- data.frame(
x = x,            # our x-values between 0 and 4
y_line = y,  # y-data with line
y_data = y_witherror   # y-data with error
)
# 6) Now, plot to check it out
plot(df)
# ggplot(data, aes(x = x)) +
#  geom_point(aes(y = y_data), alpha = 0.6, color = 'blue') +  # Plotting the data points
#  geom_line(aes(y = y_line), color = 'black') +  # Plotting the regression line                 #### --> HERE 1*
#  labs(
#    x = 'Average recent growth in personal income (%)',
#    y = "Incumbent party's vote share",
#    title = 'Hypothetical data and fitted line',
#    subtitle = 'Fitted line: y = 30 + 10x'
#  ) +
#  theme_minimal() +
#  theme(legend.position = "none") +
#    geom_smooth(aes(y = y_data), method = "lm", se = FALSE, color = "black")                    #### --> HERE *
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y_witherror ~ x, data=df)
print(M2)
coef(M2)
abline(coef(M2))
M3 <- stan_glm(y ~ x, data=df)
print(M3)
M12a <- stan_glm(y_data ~ x, data = data, refresh = 0)
print(M12a) # We see 3.9 under sigma indeed
M13a <- stan_glm(y_line ~ x, data = data, refresh = 0)
