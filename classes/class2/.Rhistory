knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/paulasamide/Desktop/AU/semester 2/methods/resources/classes")
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x + residual_sd
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
#set.seed(1)
#nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x + residual_sd
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
#set.seed(1)
#nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
#y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y)
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/paulasamide/Desktop/AU/semester 2/methods/resources/classes")
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
library(rstanarm)
# Load the rest
library(pacman)
pacman::p_load(tidyverse,
ggpubr,
ggplot2,
stringr) # this time I'm just giving you the code
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x + residual_sd
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
#set.seed(1)
#nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
#y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y)
# 6) Now, plot to check it out
plot(df)
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y ~ x, data=df)
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x + residual_sd
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
#set.seed(1)
#nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
#y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y)
# 6) Now, plot to check it out
plot(df)
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y ~ x, data=df)
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x + residual_sd
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
#set.seed(1)
#nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
#y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y)
# 6) Now, plot to check it out
plot(df)
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
# M2 <- stan_glm(y ~ x, data=df)
# print(M2) ## you can see that it deviates a little bit from the 'original' data - bc it takes error into account <3
# to include it in the plot
#coef(M2)
#abline(coef(M2))
#### 1) define the regression model parameters (intercept, slope, error)
intercept <- 30
slope <- 10
residual_sd <- 3.9
# 2) generate x-values in the required range
x <- seq(0, 4, length.out = 20)
# 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line
y <- intercept + slope * x
# 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.
set.seed(1)
nd_random_error <- rnorm(20, mean = 0, sd = residual_sd)
y_witherror <- y+nd_random_error
# 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot
df <- tibble(x = x, y = y_witherror)
# 6) Now, plot to check it out
plot(df)
# 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data.
M2 <- stan_glm(y_witherror ~ x, data=df)
print(M2) ## you can see that it deviates a little bit from the 'original' data - bc it takes error into account <3
# to include it in the plot
coef(M2)
abline(coef(M2))
